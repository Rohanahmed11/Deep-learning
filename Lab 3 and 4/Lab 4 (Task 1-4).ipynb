{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59c5b14-0c2d-497d-ad4c-74d6137ec54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize data (important for neural networks)\n",
    "X_train, X_test = X_train / X_train.max(), X_test / X_test.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2108e57f-9ef8-467f-877a-2676dce734ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5079 - loss: 0.7304 - val_accuracy: 0.4650 - val_loss: 0.6980\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5161 - loss: 0.6917 - val_accuracy: 0.7450 - val_loss: 0.6856\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6637 - loss: 0.6853 - val_accuracy: 0.4800 - val_loss: 0.6844\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6034 - loss: 0.6820 - val_accuracy: 0.5550 - val_loss: 0.6781\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5794 - loss: 0.6715 - val_accuracy: 0.7100 - val_loss: 0.6639\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7595 - loss: 0.6655 - val_accuracy: 0.6500 - val_loss: 0.6621\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6822 - loss: 0.6584 - val_accuracy: 0.7000 - val_loss: 0.6495\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6814 - loss: 0.6405 - val_accuracy: 0.7350 - val_loss: 0.6259\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7881 - loss: 0.6258 - val_accuracy: 0.7700 - val_loss: 0.6139\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8231 - loss: 0.6055 - val_accuracy: 0.8250 - val_loss: 0.5863\n",
      "Training Accuracy with Sigmoid: 0.8313\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predictions on Test Data with Sigmoid: [[0.527413  ]\n",
      " [0.531767  ]\n",
      " [0.48672596]\n",
      " [0.5829036 ]\n",
      " [0.62749815]]\n"
     ]
    }
   ],
   "source": [
    "def build_model(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=20, activation=activation_function))  # First layer\n",
    "    model.add(Dense(32, activation=activation_function))  # Second layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model with 'sigmoid' activation function\n",
    "model_sigmoid = build_model('sigmoid')\n",
    "history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print training accuracy and model predictions\n",
    "print(f\"Training Accuracy with Sigmoid: {history_sigmoid.history['accuracy'][-1]:.4f}\")\n",
    "print(\"Predictions on Test Data with Sigmoid:\", model_sigmoid.predict(X_test[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7135cf4b-6cca-4dc4-bc07-ad44ef31f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5763 - loss: 0.6804 - val_accuracy: 0.7900 - val_loss: 0.6249\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8448 - loss: 0.6083 - val_accuracy: 0.8200 - val_loss: 0.5387\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8705 - loss: 0.5128 - val_accuracy: 0.8300 - val_loss: 0.4471\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8504 - loss: 0.4159 - val_accuracy: 0.8300 - val_loss: 0.3945\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8838 - loss: 0.3490 - val_accuracy: 0.8450 - val_loss: 0.3690\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8769 - loss: 0.3247 - val_accuracy: 0.8600 - val_loss: 0.3607\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8814 - loss: 0.3235 - val_accuracy: 0.8600 - val_loss: 0.3621\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8754 - loss: 0.3050 - val_accuracy: 0.8500 - val_loss: 0.3670\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8827 - loss: 0.2880 - val_accuracy: 0.8450 - val_loss: 0.3656\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.2788 - val_accuracy: 0.8500 - val_loss: 0.3641\n",
      "Training Accuracy with ReLU: 0.8875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions on Test Data with ReLU: [[0.6252326 ]\n",
      " [0.73671997]\n",
      " [0.53774977]\n",
      " [0.91846514]\n",
      " [0.9778912 ]]\n"
     ]
    }
   ],
   "source": [
    "# Build and train the model with 'relu' activation function\n",
    "model_relu = build_model('relu')\n",
    "history_relu = model_relu.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print training accuracy and model predictions\n",
    "print(f\"Training Accuracy with ReLU: {history_relu.history['accuracy'][-1]:.4f}\")\n",
    "print(\"Predictions on Test Data with ReLU:\", model_relu.predict(X_test[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc08b4b-a2ba-479b-8d83-21f529e95f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6865 - loss: 0.6725 - val_accuracy: 0.7250 - val_loss: 0.6357\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7806 - loss: 0.6102 - val_accuracy: 0.7950 - val_loss: 0.5476\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8160 - loss: 0.5034 - val_accuracy: 0.8400 - val_loss: 0.4320\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8735 - loss: 0.3743 - val_accuracy: 0.8400 - val_loss: 0.3829\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9026 - loss: 0.3047 - val_accuracy: 0.8550 - val_loss: 0.3726\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8874 - loss: 0.3115 - val_accuracy: 0.8650 - val_loss: 0.3670\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.3127 - val_accuracy: 0.8500 - val_loss: 0.3841\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8705 - loss: 0.2882 - val_accuracy: 0.8650 - val_loss: 0.3706\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9078 - loss: 0.2610 - val_accuracy: 0.8650 - val_loss: 0.3827\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9026 - loss: 0.2593 - val_accuracy: 0.8650 - val_loss: 0.3705\n",
      "Training Accuracy with Extra Layer: 0.9000\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000213D079EFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predictions on Test Data with Extra Layer: [[0.7494761 ]\n",
      " [0.8895163 ]\n",
      " [0.68105596]\n",
      " [0.94717234]\n",
      " [0.9870185 ]]\n"
     ]
    }
   ],
   "source": [
    "def build_model_with_additional_layer(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=20, activation=activation_function))  # First layer\n",
    "    model.add(Dense(32, activation=activation_function))  # Second layer\n",
    "    model.add(Dense(16, activation=activation_function))  # Third layer (new hidden layer)\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model with an extra hidden layer and 'relu' activation function\n",
    "model_extra_layer = build_model_with_additional_layer('relu')\n",
    "history_extra_layer = model_extra_layer.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print training accuracy and model predictions\n",
    "print(f\"Training Accuracy with Extra Layer: {history_extra_layer.history['accuracy'][-1]:.4f}\")\n",
    "print(\"Predictions on Test Data with Extra Layer:\", model_extra_layer.predict(X_test[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d87f37-2114-4048-8cee-1105fca7d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7276 - loss: 0.6621 - val_accuracy: 0.8150 - val_loss: 0.5500\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8620 - loss: 0.4965 - val_accuracy: 0.8400 - val_loss: 0.3903\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8788 - loss: 0.3155 - val_accuracy: 0.8550 - val_loss: 0.3664\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8811 - loss: 0.2869 - val_accuracy: 0.8700 - val_loss: 0.3725\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8789 - loss: 0.2969 - val_accuracy: 0.8550 - val_loss: 0.3587\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8757 - loss: 0.3094 - val_accuracy: 0.8550 - val_loss: 0.3683\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9128 - loss: 0.2524 - val_accuracy: 0.8700 - val_loss: 0.3701\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9096 - loss: 0.2527 - val_accuracy: 0.8450 - val_loss: 0.4029\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9077 - loss: 0.2465 - val_accuracy: 0.8500 - val_loss: 0.3853\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9000 - loss: 0.2463 - val_accuracy: 0.8550 - val_loss: 0.3866\n",
      "Training Accuracy with More Neurons: 0.9150\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000213D76047C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions on Test Data with More Neurons: [[0.52764845]\n",
      " [0.8376863 ]\n",
      " [0.3909491 ]\n",
      " [0.9630439 ]\n",
      " [0.97905463]]\n"
     ]
    }
   ],
   "source": [
    "def build_model_with_more_neurons(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=20, activation=activation_function))  # First layer with more neurons\n",
    "    model.add(Dense(64, activation=activation_function))  # Second layer with more neurons\n",
    "    model.add(Dense(32, activation=activation_function))  # Third layer with more neurons\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model with more neurons and 'relu' activation function\n",
    "model_more_neurons = build_model_with_more_neurons('relu')\n",
    "history_more_neurons = model_more_neurons.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print training accuracy and model predictions\n",
    "print(f\"Training Accuracy with More Neurons: {history_more_neurons.history['accuracy'][-1]:.4f}\")\n",
    "print(\"Predictions on Test Data with More Neurons:\", model_more_neurons.predict(X_test[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdde8fd-ae40-4179-b0f7-00a4ea95093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "---- Task 1: Activation Function ----\n",
      "Training Accuracy with Sigmoid: 0.7875\n",
      "Loss with Sigmoid: 0.5914\n",
      "Predictions with Sigmoid: [[0.5705516 ]\n",
      " [0.5321364 ]\n",
      " [0.4805121 ]\n",
      " [0.59721804]\n",
      " [0.65628314]]\n",
      "Training Accuracy with ReLU: 0.8863\n",
      "Loss with ReLU: 0.2829\n",
      "Predictions with ReLU: [[0.6848479 ]\n",
      " [0.81932795]\n",
      " [0.31171837]\n",
      " [0.9294936 ]\n",
      " [0.9847395 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "---- Task 2: Add One More Hidden Layer ----\n",
      "Training Accuracy with Extra Layer: 0.9050\n",
      "Loss with Extra Layer: 0.2573\n",
      "Predictions with Extra Layer: [[0.58370197]\n",
      " [0.93279755]\n",
      " [0.36744016]\n",
      " [0.9316753 ]\n",
      " [0.955146  ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "---- Task 3: Add More Neurons ----\n",
      "Training Accuracy with More Neurons: 0.9187\n",
      "Loss with More Neurons: 0.2233\n",
      "Predictions with More Neurons: [[0.83962333]\n",
      " [0.8744135 ]\n",
      " [0.46353638]\n",
      " [0.97032416]\n",
      " [0.9904847 ]]\n",
      "\n",
      "---- Final Summary ----\n",
      "Sigmoid Activation: Accuracy = 0.7875, Loss = 0.5914\n",
      "ReLU Activation: Accuracy = 0.8863, Loss = 0.2829\n",
      "Extra Hidden Layer: Accuracy = 0.9050, Loss = 0.2573\n",
      "More Neurons: Accuracy = 0.9187, Loss = 0.2233\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize data (important for neural networks)\n",
    "X_train, X_test = X_train / X_train.max(), X_test / X_test.max()\n",
    "\n",
    "# Function to build the model with specified activation function\n",
    "def build_model(activation_function, additional_layers=False, more_neurons=False):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if more_neurons:\n",
    "        # More neurons in the first layer\n",
    "        model.add(Dense(128, input_dim=20, activation=activation_function))\n",
    "        model.add(Dense(64, activation=activation_function))\n",
    "        model.add(Dense(32, activation=activation_function))\n",
    "    else:\n",
    "        # Normal configuration with less neurons\n",
    "        model.add(Dense(64, input_dim=20, activation=activation_function))  # First layer\n",
    "        model.add(Dense(32, activation=activation_function))  # Second layer\n",
    "    \n",
    "    if additional_layers:\n",
    "        # Adding an additional hidden layer\n",
    "        model.add(Dense(16, activation=activation_function))  # Extra layer\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train the model and print required metrics\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    # Training accuracy\n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    # Loss value\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Predictions on the first few test data points\n",
    "    predictions = model.predict(X_test[:5])\n",
    "    \n",
    "    return train_accuracy, loss, predictions\n",
    "\n",
    "# Task 1: Using Sigmoid vs ReLU Activation Function\n",
    "\n",
    "# Model with Sigmoid\n",
    "model_sigmoid = build_model('sigmoid')\n",
    "train_accuracy_sigmoid, loss_sigmoid, predictions_sigmoid = train_and_evaluate_model(model_sigmoid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model with ReLU\n",
    "model_relu = build_model('relu')\n",
    "train_accuracy_relu, loss_relu, predictions_relu = train_and_evaluate_model(model_relu, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"---- Task 1: Activation Function ----\")\n",
    "print(f\"Training Accuracy with Sigmoid: {train_accuracy_sigmoid:.4f}\")\n",
    "print(f\"Loss with Sigmoid: {loss_sigmoid:.4f}\")\n",
    "print(\"Predictions with Sigmoid:\", predictions_sigmoid)\n",
    "\n",
    "print(f\"Training Accuracy with ReLU: {train_accuracy_relu:.4f}\")\n",
    "print(f\"Loss with ReLU: {loss_relu:.4f}\")\n",
    "print(\"Predictions with ReLU:\", predictions_relu)\n",
    "\n",
    "# Task 2: Add One More Hidden Layer\n",
    "\n",
    "# Model with additional layer (ReLU)\n",
    "model_extra_layer = build_model('relu', additional_layers=True)\n",
    "train_accuracy_extra_layer, loss_extra_layer, predictions_extra_layer = train_and_evaluate_model(model_extra_layer, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n---- Task 2: Add One More Hidden Layer ----\")\n",
    "print(f\"Training Accuracy with Extra Layer: {train_accuracy_extra_layer:.4f}\")\n",
    "print(f\"Loss with Extra Layer: {loss_extra_layer:.4f}\")\n",
    "print(\"Predictions with Extra Layer:\", predictions_extra_layer)\n",
    "\n",
    "# Task 3: Add More Neurons\n",
    "\n",
    "# Model with more neurons (ReLU)\n",
    "model_more_neurons = build_model('relu', more_neurons=True)\n",
    "train_accuracy_more_neurons, loss_more_neurons, predictions_more_neurons = train_and_evaluate_model(model_more_neurons, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n---- Task 3: Add More Neurons ----\")\n",
    "print(f\"Training Accuracy with More Neurons: {train_accuracy_more_neurons:.4f}\")\n",
    "print(f\"Loss with More Neurons: {loss_more_neurons:.4f}\")\n",
    "print(\"Predictions with More Neurons:\", predictions_more_neurons)\n",
    "\n",
    "# Final Summary - Compare Accuracy and Loss\n",
    "print(\"\\n---- Final Summary ----\")\n",
    "print(f\"Sigmoid Activation: Accuracy = {train_accuracy_sigmoid:.4f}, Loss = {loss_sigmoid:.4f}\")\n",
    "print(f\"ReLU Activation: Accuracy = {train_accuracy_relu:.4f}, Loss = {loss_relu:.4f}\")\n",
    "print(f\"Extra Hidden Layer: Accuracy = {train_accuracy_extra_layer:.4f}, Loss = {loss_extra_layer:.4f}\")\n",
    "print(f\"More Neurons: Accuracy = {train_accuracy_more_neurons:.4f}, Loss = {loss_more_neurons:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d49ae-2a7e-4a13-b935-cb0119e2e630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
